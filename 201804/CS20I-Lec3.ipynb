{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-fe8bbe7e1aea>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data/mnist', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import examples.utils as utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/mnist/train-images-idx3-ubyte.gz already exists\n",
      "data/mnist/train-labels-idx1-ubyte.gz already exists\n",
      "data/mnist/t10k-images-idx3-ubyte.gz already exists\n",
      "data/mnist/t10k-labels-idx1-ubyte.gz already exists\n"
     ]
    }
   ],
   "source": [
    "mnist_path = 'data/mnist'\n",
    "utils.download_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "n_train = 60000\n",
    "n_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val,test = utils.read_mnist(mnist_path, flatten = True)\n",
    "tf.reset_default_graph() #Useful to reset graph, because else tf throws\n",
    "                         # errors with named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train[0]) #55k Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train[1]) # one-hot labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Use tf to create the dataset\n",
    "- Need to one set for training and test.\n",
    "- Setting shuffle, shuffles the data\n",
    "'''\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000)\n",
    "test_data = tf.data.Dataset.from_tensor_slices(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilize the data with specific batch size\n",
    "train_data = train_data.batch(batch_size)\n",
    "test_data = test_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Next step:- Create iterator to get samples from the two datasets\n",
    "- In linear regression-> used only train set. hence can create one iterator and draw samples from dataset.\n",
    "- if train and test set and if we have one iterator for each date set, then need to build graph for each iterator\n",
    "- better way: Create one signle iterator and initialize it with a dataset, when we need to draw from it.\n",
    "\n",
    "Iterator represents thse state of iterating through a dataset\n",
    "Iterator.from_structure :- creates a new uninitialzed iterator with a \n",
    " given structure\n",
    " \n",
    " This iterator constructing method can be used to create an iterator that\n",
    " is reusable with many different datasets\n",
    " \n",
    " To initialize it with particular dataset:- Iterator.make_initializer(dataset)\n",
    "'''\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
    "img,label = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the itertor with two different datasets\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "test_init = iterator.make_initializer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note the shapes are different from Andrew's class\n",
    "# X = img = (n x d) = (n0 x n1) = 55000 x 784\n",
    "# w = (n1 x n2)\n",
    "# b = (1 x n2)\n",
    "w = tf.get_variable(shape = (784,10), name = \"weights\", initializer = tf.random_normal_initializer(0,0.01))\n",
    "b = tf.get_variable(shape = (1,10), name = \"bias\", initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(img,w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-ad8c916cb769>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loss\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = logits, name = \"entropy\")\n",
    "loss = tf.reduce_mean(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set accuracty\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds,1),tf.argmax(label,1))\n",
    "'''\n",
    "tf.argmax(input, axis)\n",
    "axis 1 -> index of max element in each row.\n",
    "tf.argmax(preds,1):- index of max (highest logit)\n",
    "tf.argmax(label,1):- one hot encoded hence, only 1 entry will have label 1\n",
    "'''\n",
    "\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 0 is 0.36646836445082065\n",
      "Average loss for epoch 1 is 0.29454639375556346\n",
      "Average loss for epoch 2 is 0.28577209749194077\n",
      "Average loss for epoch 3 is 0.2796337234419446\n",
      "Average loss for epoch 4 is 0.2744848436740942\n",
      "Average loss for epoch 5 is 0.27121177289721576\n",
      "Average loss for epoch 6 is 0.2698063459680524\n",
      "Average loss for epoch 7 is 0.26876733174850775\n",
      "Average loss for epoch 8 is 0.26669259085211644\n",
      "Average loss for epoch 9 is 0.2680856530402982\n",
      "Average loss for epoch 10 is 0.2629014372479084\n",
      "Average loss for epoch 11 is 0.2598228190527406\n",
      "Average loss for epoch 12 is 0.26267928609321284\n",
      "Average loss for epoch 13 is 0.26255136738682905\n",
      "Average loss for epoch 14 is 0.2599228248346684\n",
      "Average loss for epoch 15 is 0.2594548969767814\n",
      "Average loss for epoch 16 is 0.2617419119664403\n",
      "Average loss for epoch 17 is 0.2582778943139453\n",
      "Average loss for epoch 18 is 0.25696964998577915\n",
      "Average loss for epoch 19 is 0.25833764824756356\n",
      "Average loss for epoch 20 is 0.25702338677852654\n",
      "Average loss for epoch 21 is 0.25803184230313747\n",
      "Average loss for epoch 22 is 0.25664376872916556\n",
      "Average loss for epoch 23 is 0.2545026562934698\n",
      "Average loss for epoch 24 is 0.2557822357776553\n",
      "Average loss for epoch 25 is 0.25379600968471794\n",
      "Average loss for epoch 26 is 0.2554069790036179\n",
      "Average loss for epoch 27 is 0.2553101039036762\n",
      "Average loss for epoch 28 is 0.25349872389505074\n",
      "Average loss for epoch 29 is 0.25557551865660866\n",
      "Total time: 29.058072090148926\n",
      "Accuracty:0.9183\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        sess.run(train_init) #get samples from the data\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                _,l = sess.run([optimizer,loss])\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        print(\"Average loss for epoch {0} is {1}\".format(i, total_loss/n_batches))\n",
    "    print(\"Total time: {0}\".format(time.time()-start_time))\n",
    "    \n",
    "    sess.run(test_init)\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch = sess.run(accuracy)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    \n",
    "    print(\"Accuracty:{0}\".format(total_correct_preds/n_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
